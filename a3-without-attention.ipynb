{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Downloading dakshina dataset\n!yes | wget \"https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T05:19:56.315096Z","iopub.execute_input":"2025-05-20T05:19:56.315900Z","iopub.status.idle":"2025-05-20T05:20:02.706881Z","shell.execute_reply.started":"2025-05-20T05:19:56.315876Z","shell.execute_reply":"2025-05-20T05:20:02.706161Z"}},"outputs":[{"name":"stdout","text":"--2025-05-20 05:19:56--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\nResolving storage.googleapis.com (storage.googleapis.com)... 74.125.134.207, 172.217.203.207, 172.217.204.207, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|74.125.134.207|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2008340480 (1.9G) [application/x-tar]\nSaving to: ‘dakshina_dataset_v1.0.tar’\n\ndakshina_dataset_v1 100%[===================>]   1.87G   299MB/s    in 6.1s    \n\n2025-05-20 05:20:02 (313 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n\nyes: standard output: Broken pipe\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Unzipping dataset\n!yes | tar xopf dakshina_dataset_v1.0.tar","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T05:20:04.037432Z","iopub.execute_input":"2025-05-20T05:20:04.037697Z","iopub.status.idle":"2025-05-20T05:20:06.292309Z","shell.execute_reply.started":"2025-05-20T05:20:04.037674Z","shell.execute_reply":"2025-05-20T05:20:06.291302Z"}},"outputs":[{"name":"stdout","text":"yes: standard output: Broken pipe\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"wandb.login(key='49f8f505158ee3693f0cacf0a82118bd4e636e8c')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T05:20:24.927782Z","iopub.execute_input":"2025-05-20T05:20:24.928436Z","iopub.status.idle":"2025-05-20T05:20:31.099289Z","shell.execute_reply.started":"2025-05-20T05:20:24.928411Z","shell.execute_reply":"2025-05-20T05:20:31.098736Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msurendarmohan283\u001b[0m (\u001b[33msurendarmohan283-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nimport random\n\n# For reproducibility\ndef set_random_seeds(seed_value=42):\n    \"\"\"\n    Set deterministic behavior for reproducibility across runs.\n    Configures random number generators for all libraries.\n    \"\"\"\n    # Set Python's built-in random module seed\n    random.seed(seed_value)\n    \n    # Set PyTorch CPU operations seed\n    torch.manual_seed(seed_value)\n    \n    # Set PyTorch GPU operations seeds if available\n    torch.cuda.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)\n    \n    # Make CUDA operations deterministic\n    torch.backends.cudnn.deterministic = True\n    \n    # Disable CUDA backend auto-tuning\n    torch.backends.cudnn.benchmark = False\n\nclass SourceEncoder(nn.Module):\n    \"\"\"Encodes the input sequence into a context representation.\"\"\"\n    def __init__(self, vocabulary_size, embedding_dim, hidden_dim, num_layers=1, \n                 rnn_type='LSTM', dropout_rate=0.0, use_bidirectional=False):\n        super().__init__()\n        # Create embedding layer\n        self.token_embeddings = nn.Embedding(vocabulary_size, embedding_dim)\n        \n        # Store configuration\n        self.is_bidirectional = use_bidirectional\n        self.rnn_type = rnn_type\n        self.num_layers = num_layers\n        self.hidden_dim = hidden_dim\n        \n        # Calculate output dimension\n        self.context_dim = hidden_dim * 2 if use_bidirectional else hidden_dim\n        \n        # Select RNN implementation\n        rnn_options = {'LSTM': nn.LSTM, 'GRU': nn.GRU, 'RNN': nn.RNN}\n        rnn_class = rnn_options[rnn_type]\n        \n        # Create recurrent layer\n        self.recurrent = rnn_class(\n            input_size=embedding_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            dropout=dropout_rate if num_layers > 1 else 0.0,\n            batch_first=True,\n            bidirectional=use_bidirectional\n        )\n\n    def forward(self, input_sequence, sequence_lengths):\n        \"\"\"\n        Process source sequence and return context representation.\n        \n        Args:\n            input_sequence: Tensor of token IDs [batch_size, seq_len]\n            sequence_lengths: Tensor of sequence lengths [batch_size]\n            \n        Returns:\n            context_states: Sequence of context states [batch_size, seq_len, hidden_dim*dirs]\n            final_state: Final hidden state(s) for decoder initialization\n        \"\"\"\n        # Get embeddings for input tokens\n        embedded_seq = self.token_embeddings(input_sequence)\n        \n        # Pack sequence to handle variable lengths\n        packed_input = pack_padded_sequence(\n            embedded_seq, sequence_lengths.cpu(), \n            batch_first=True, enforce_sorted=False\n        )\n        \n        # Pass through recurrent network\n        packed_outputs, final_states = self.recurrent(packed_input)\n        \n        # Unpack outputs\n        context_states, _ = pad_packed_sequence(packed_outputs, batch_first=True)\n        \n        # Process final states for bidirectional case\n        if self.is_bidirectional:\n            if self.rnn_type == 'LSTM':\n                # Process both hidden and cell states\n                h_final, c_final = final_states\n                # Average forward and backward states for each layer\n                h_final = torch.mean(\n                    torch.stack([h_final[:self.num_layers], h_final[self.num_layers:]]), dim=0\n                )\n                c_final = torch.mean(\n                    torch.stack([c_final[:self.num_layers], c_final[self.num_layers:]]), dim=0\n                )\n                final_states = (h_final, c_final)\n            else:\n                # For GRU/RNN, process only hidden state\n                final_states = torch.mean(\n                    torch.stack([final_states[:self.num_layers], final_states[self.num_layers:]]), dim=0\n                )\n                \n        return context_states, final_states\n\n\nclass AdditiveAttention(nn.Module):\n    \"\"\"\n    Implements Bahdanau-style additive attention mechanism.\n    Calculates attention weights based on encoder states and decoder state.\n    \"\"\"\n    def __init__(self, encoder_dim, decoder_dim):\n        super().__init__()\n        # Scoring network components\n        self.score_projection = nn.Linear(encoder_dim + decoder_dim, decoder_dim)\n        self.weight_vector = nn.Linear(decoder_dim, 1, bias=False)\n\n    def forward(self, decoder_state, encoder_states, attention_mask):\n        \"\"\"\n        Calculate attention weights for each encoder state.\n        \n        Args:\n            decoder_state: Current decoder state [batch_size, decoder_dim]\n            encoder_states: All encoder states [batch_size, seq_len, encoder_dim]\n            attention_mask: Boolean mask for padding [batch_size, seq_len]\n            \n        Returns:\n            attention_weights: Softmax distribution over encoder states [batch_size, seq_len]\n        \"\"\"\n        # Get sequence dimensions\n        batch_size, seq_len, _ = encoder_states.size()\n        \n        # Repeat decoder state for each position in sequence\n        expanded_state = decoder_state.unsqueeze(1).expand(-1, seq_len, -1)\n        \n        # Concatenate encoder and decoder states\n        combined = torch.cat((expanded_state, encoder_states), dim=2)\n        \n        # Calculate attention scores\n        attention_features = torch.tanh(self.score_projection(combined))\n        attention_scores = self.weight_vector(attention_features).squeeze(2)\n        \n        # Apply mask to prevent attention to padding\n        attention_scores = attention_scores.masked_fill(~attention_mask, float('-inf'))\n        \n        # Get probability distribution\n        attention_weights = torch.softmax(attention_scores, dim=1)\n        \n        return attention_weights\n\n\nclass TargetDecoder(nn.Module):\n    \"\"\"\n    Decoder for generating target sequence.\n    \n    Supports two operating modes:\n    1. With attention (using context from encoder)\n    2. Without attention (simple RNN decoder)\n    \"\"\"\n    def __init__(self, target_vocab_size, embedding_dim, encoder_dim, decoder_dim,\n                 num_layers=1, rnn_type=\"LSTM\", dropout_rate=0.0, with_attention=True):\n        super().__init__()\n        \n        # Configuration\n        self.with_attention = with_attention\n        self.rnn_type = rnn_type\n        \n        # Token embedding layer\n        self.token_embeddings = nn.Embedding(target_vocab_size, embedding_dim)\n        \n        # Determine input dimensions based on attention usage\n        if with_attention:\n            # Create attention mechanism\n            self.attention_module = AdditiveAttention(encoder_dim, decoder_dim)\n            \n            # When using attention, we concatenate context vector with token embedding\n            recurrent_input_dim = embedding_dim + encoder_dim\n            \n            # Output combines decoder state, context vector, and current embedding\n            output_input_dim = decoder_dim + encoder_dim + embedding_dim\n        else:\n            # Without attention, just use embeddings as input\n            recurrent_input_dim = embedding_dim\n            \n            # Output combines decoder state and current embedding\n            output_input_dim = decoder_dim + embedding_dim\n\n        # Select appropriate RNN implementation\n        rnn_options = {\"LSTM\": nn.LSTM, \"GRU\": nn.GRU, \"RNN\": nn.RNN}\n        selected_rnn = rnn_options[rnn_type]\n        \n        # Create recurrent network\n        self.recurrent = selected_rnn(\n            input_size=recurrent_input_dim,\n            hidden_size=decoder_dim,\n            num_layers=num_layers,\n            dropout=dropout_rate if num_layers > 1 else 0.0,\n            batch_first=True\n        )\n        \n        # Output projection\n        self.output_projection = nn.Linear(output_input_dim, target_vocab_size)\n\n    def forward(self, current_token, hidden_state, encoder_states, padding_mask):\n        \"\"\"\n        Process one decoding step.\n        \n        Args:\n            current_token: Current input token ids [batch_size]\n            hidden_state: Previous decoder hidden state\n            encoder_states: All encoder hidden states [batch_size, src_len, encoder_dim]\n            padding_mask: Mask for padding in source [batch_size, src_len]\n            \n        Returns:\n            token_logits: Probability distribution over target vocabulary\n            new_hidden: Updated hidden state\n            attention_weights: Attention distribution over source (or None)\n        \"\"\"\n        # Embed current token and add time dimension\n        token_embedding = self.token_embeddings(current_token).unsqueeze(1)  # [B,1,E]\n\n        # Process differently based on attention mode\n        if self.with_attention:\n            # Extract current hidden state for attention\n            if self.rnn_type == 'LSTM':\n                # For LSTM, get the hidden state (not cell state)\n                current_decoder_state = hidden_state[0][-1]\n            else:\n                # For GRU/RNN, directly use the hidden state\n                current_decoder_state = hidden_state[-1]\n            \n            # Calculate attention weights\n            attention_weights = self.attention_module(\n                current_decoder_state, encoder_states, padding_mask\n            )\n            \n            # Get context vector by weighted sum of encoder states\n            context_vector = torch.bmm(\n                attention_weights.unsqueeze(1), encoder_states\n            )\n            \n            # Combine token embedding with context for recurrent input\n            recurrent_input = torch.cat((token_embedding, context_vector), dim=2)\n        else:\n            # Without attention, just use token embedding\n            context_vector = None\n            attention_weights = None\n            recurrent_input = token_embedding\n\n        # Process through RNN\n        recurrent_output, new_hidden = self.recurrent(recurrent_input, hidden_state)\n        \n        # Remove time dimension\n        recurrent_output = recurrent_output.squeeze(1)  # [B,H]\n        token_embedding = token_embedding.squeeze(1)    # [B,E]\n\n        # Generate output logits\n        if self.with_attention:\n            # Combine decoder output, context vector, and token embedding\n            context_vector = context_vector.squeeze(1)  # [B,C]\n            token_logits = self.output_projection(\n                torch.cat((recurrent_output, context_vector, token_embedding), dim=1)\n            )\n        else:\n            # Combine decoder output and token embedding\n            token_logits = self.output_projection(\n                torch.cat((recurrent_output, token_embedding), dim=1)\n            )\n\n        return token_logits, new_hidden, attention_weights\n\n\nclass TransliterationModel(nn.Module):\n    \"\"\"\n    Sequence-to-sequence model for transliteration tasks.\n    Combines encoder and decoder with optional attention.\n    \"\"\"\n    def __init__(self, source_encoder, target_decoder, padding_idx, device_name='cpu'):\n        super().__init__()\n        self.source_encoder = source_encoder\n        self.target_decoder = target_decoder\n        self.padding_idx = padding_idx\n        self.device = device_name\n\n    def forward(self, source_tokens, source_lengths, target_tokens, teacher_forcing_prob=0.5):\n        \"\"\"\n        Training forward pass with teacher forcing.\n        \n        Args:\n            source_tokens: Input token IDs [batch_size, src_len]\n            source_lengths: Length of each input sequence [batch_size]\n            target_tokens: Target token IDs [batch_size, tgt_len]\n            teacher_forcing_prob: Probability of using teacher forcing\n            \n        Returns:\n            prediction_logits: Logits for each target position [batch_size, tgt_len-1, vocab_size]\n        \"\"\"\n        # Encode source sequence\n        encoder_states, encoder_final = self.source_encoder(source_tokens, source_lengths)\n        \n        # Create mask for attention\n        attention_mask = (source_tokens != self.padding_idx)\n        \n        # Prepare storage for decoder outputs\n        batch_size, target_length = target_tokens.size()\n        vocab_size = self.target_decoder.output_projection.out_features\n        prediction_logits = torch.zeros(batch_size, target_length-1, vocab_size, device=self.device)\n        \n        # Initial input is the start token\n        current_input = target_tokens[:, 0]  # <sos> token\n        \n        # Generate each target token\n        for timestep in range(1, target_length):\n            # Get prediction for current position\n            step_output, encoder_final, _ = self.target_decoder(\n                current_input, encoder_final, encoder_states, attention_mask\n            )\n            \n            # Store prediction\n            prediction_logits[:, timestep-1] = step_output\n            \n            # Determine next input (teacher forcing vs. predicted)\n            use_teacher_forcing = random.random() < teacher_forcing_prob\n            if use_teacher_forcing:\n                # Use ground truth as next input\n                current_input = target_tokens[:, timestep]\n            else:\n                # Use model's prediction as next input\n                current_input = step_output.argmax(1)\n                \n        return prediction_logits\n\n    def generate_sequence(self, source_tokens, source_lengths, target_vocab, max_length=50):\n        \"\"\"\n        Generate target sequence using greedy decoding.\n        \n        Args:\n            source_tokens: Input token IDs [batch_size, src_len]\n            source_lengths: Length of each input sequence [batch_size]\n            target_vocab: Target vocabulary object\n            max_length: Maximum generation length\n            \n        Returns:\n            generated_tokens: Sequence of generated token IDs [batch_size, gen_len]\n        \"\"\"\n        # Encode source sequence\n        encoder_states, encoder_final = self.source_encoder(source_tokens, source_lengths)\n        \n        # Create mask for attention\n        attention_mask = (source_tokens != self.padding_idx)\n        \n        # Initialize generation with start token\n        batch_size = source_tokens.size(0)\n        current_input = torch.full(\n            (batch_size,), target_vocab.sos_idx, \n            device=self.device, dtype=torch.long\n        )\n        \n        # Store for generated tokens\n        generated_tokens = []\n        \n        # Generate sequence\n        for _ in range(max_length):\n            # Get prediction for current position\n            step_output, encoder_final, _ = self.target_decoder(\n                current_input, encoder_final, encoder_states, attention_mask\n            )\n            \n            # Select most likely token\n            current_input = step_output.argmax(1)\n            \n            # Store generated token\n            generated_tokens.append(current_input.unsqueeze(1))\n            \n            # Check if all sequences have generated end token\n            if (current_input == target_vocab.eos_idx).all():\n                break\n                \n        # Combine all generated tokens\n        return torch.cat(generated_tokens, dim=1)\n\n\n# Character-level vocabulary handling\nclass CharacterVocabulary:\n    \"\"\"\n    Character-level vocabulary for transliteration tasks.\n    Maps characters to integer indices and vice versa.\n    \"\"\"\n    def __init__(self, character_list=None, special_tokens=['<pad>', '<sos>', '<eos>', '<unk>']):\n        self.special_tokens = special_tokens\n        \n        # Initialize mapping dictionaries\n        self.index_to_char = list(special_tokens) + (character_list or [])\n        self.char_to_index = {char: idx for idx, char in enumerate(self.index_to_char)}\n    \n    @classmethod\n    def build_from_text_corpus(cls, text_corpus):\n        \"\"\"\n        Build vocabulary from a corpus of texts.\n        \n        Args:\n            text_corpus: List of text strings\n            \n        Returns:\n            CharacterVocabulary: New vocabulary instance\n        \"\"\"\n        # Collect unique characters from all texts\n        unique_chars = sorted(set(char for text in text_corpus for char in text))\n        return cls(character_list=unique_chars)\n    \n    @classmethod\n    def build_from_data_file(cls, file_path, source_col='src', target_col='trg', file_format='tsv'):\n        \"\"\"\n        Build vocabulary from a data file.\n        \n        Args:\n            file_path: Path to the data file\n            source_col: Name/index of source column\n            target_col: Name/index of target column\n            file_format: File format ('tsv' or 'csv')\n            \n        Returns:\n            CharacterVocabulary: New vocabulary instance\n        \"\"\"\n        texts = []\n        \n        if file_format == 'csv':\n            import pandas as pd\n            df = pd.read_csv(file_path, header=None, names=[source_col, target_col])\n            texts = df[source_col].dropna().tolist() + df[target_col].dropna().tolist()\n        else:  # tsv\n            with open(file_path, encoding='utf-8') as f:\n                for line in f:\n                    parts = line.strip().split('\\t')\n                    if len(parts) >= 2:\n                        texts.extend([parts[0], parts[1]])\n        \n        return cls.build_from_text_corpus(texts)\n    \n    def save(self, file_path):\n        \"\"\"Save vocabulary to JSON file.\"\"\"\n        import json\n        with open(file_path, 'w', encoding='utf-8') as f:\n            json.dump(self.index_to_char, f, ensure_ascii=False)\n    \n    @classmethod\n    def load(cls, file_path):\n        \"\"\"Load vocabulary from JSON file.\"\"\"\n        import json\n        with open(file_path, encoding='utf-8') as f:\n            index_to_char = json.load(f)\n        \n        # Create instance with empty character list\n        instance = cls(character_list=[])\n        \n        # Replace mapping dictionaries\n        instance.index_to_char = index_to_char\n        instance.char_to_index = {char: idx for idx, char in enumerate(index_to_char)}\n        \n        return instance\n    \n    def convert_text_to_indices(self, text, add_start=False, add_end=False):\n        \"\"\"\n        Convert text string to sequence of token indices.\n        \n        Args:\n            text: Input text string\n            add_start: Whether to add start-of-sequence token\n            add_end: Whether to add end-of-sequence token\n            \n        Returns:\n            list: Sequence of token indices\n        \"\"\"\n        indices = []\n        \n        # Add start token if requested\n        if add_start:\n            indices.append(self.char_to_index['<sos>'])\n        \n        # Convert each character to its index\n        for char in text:\n            # Use unknown token index for out-of-vocabulary characters\n            indices.append(self.char_to_index.get(char, self.char_to_index['<unk>']))\n        \n        # Add end token if requested\n        if add_end:\n            indices.append(self.char_to_index['<eos>'])\n            \n        return indices\n    \n    def convert_indices_to_text(self, indices, remove_special=True, join_chars=True):\n        \"\"\"\n        Convert sequence of indices back to text.\n        \n        Args:\n            indices: Sequence of token indices\n            remove_special: Whether to remove special tokens\n            join_chars: Whether to join characters into a string\n            \n        Returns:\n            str or list: Decoded text as string or character list\n        \"\"\"\n        # Convert tensor to list if needed\n        if hasattr(indices, 'tolist'):\n            indices = indices.tolist()\n        \n        # Convert indices to characters, filtering out-of-range indices\n        characters = [self.index_to_char[idx] for idx in indices if idx < len(self.index_to_char)]\n        \n        # Remove special tokens if requested\n        if remove_special:\n            characters = [char for char in characters if char not in self.special_tokens]\n        \n        # Return as string or list\n        return ''.join(characters) if join_chars else characters\n    \n    def batch_decode(self, batch_indices, remove_special=True):\n        \"\"\"\n        Decode a batch of index sequences.\n        \n        Args:\n            batch_indices: Batch of index sequences\n            remove_special: Whether to remove special tokens\n            \n        Returns:\n            list: List of decoded strings\n        \"\"\"\n        return [\n            self.convert_indices_to_text(sequence, remove_special=remove_special) \n            for sequence in batch_indices\n        ]\n    \n    def get_statistics(self):\n        \"\"\"Get vocabulary statistics.\"\"\"\n        return {\n            'total_size': len(self.index_to_char),\n            'special_token_count': len(self.special_tokens),\n            'character_count': len(self.index_to_char) - len(self.special_tokens)\n        }\n    \n    def __len__(self):\n        return len(self.index_to_char)\n    \n    # Convenience properties for special token indices\n    @property\n    def pad_idx(self): \n        return self.char_to_index['<pad>']\n    \n    @property\n    def sos_idx(self): \n        return self.char_to_index['<sos>']\n    \n    @property\n    def eos_idx(self): \n        return self.char_to_index['<eos>']\n    \n    @property\n    def unk_idx(self): \n        return self.char_to_index['<unk>']\n    \n    @property\n    def size(self): \n        return len(self.index_to_char)\n\n\n# Data loading functionality\nimport os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nimport pandas as pd\nimport pickle\n\n\nclass TransliterationDataset(Dataset):\n    \"\"\"Dataset for transliteration tasks.\"\"\"\n    \n    def __init__(self, data_path, source_vocab, target_vocab, dataset_type='dakshina'):\n        \"\"\"\n        Initialize dataset.\n        \n        Args:\n            data_path: Path to the data file\n            source_vocab: Source vocabulary\n            target_vocab: Target vocabulary\n            dataset_type: Dataset format\n        \"\"\"\n        self.examples = []\n        self.dataset_type = dataset_type\n        \n        if dataset_type == 'dakshina':\n            # Load examples from Dakshina format (tab-separated)\n            for source_text, target_text in self._read_dakshina_format(data_path):\n                # Convert text to indices and prepare tensors\n                source_indices = source_vocab.convert_text_to_indices(\n                    source_text, add_start=True, add_end=True\n                )\n                target_indices = target_vocab.convert_text_to_indices(\n                    target_text, add_start=True, add_end=True\n                )\n                \n                # Store as tensors\n                self.examples.append((\n                    torch.tensor(source_indices, dtype=torch.long),\n                    torch.tensor(target_indices, dtype=torch.long)\n                ))\n        else:\n            raise ValueError(f\"Unsupported dataset type: {dataset_type}\")\n    \n    def _read_dakshina_format(self, file_path):\n        \"\"\"Read Dakshina format file (tab-separated).\"\"\"\n        with open(file_path, encoding='utf-8') as f:\n            for line in f:\n                parts = line.strip().split('\\t')\n                if len(parts) >= 2:\n                    # In Dakshina, format is: native_script, latin_script\n                    yield parts[1], parts[0]\n    \n    def __len__(self):\n        return len(self.examples)\n    \n    def __getitem__(self, idx):\n        return self.examples[idx]\n\n\ndef collate_with_padding(batch, source_vocab, target_vocab):\n    \"\"\"\n    Collate function to handle variable-length sequences.\n    Pads sequences to the maximum length in the batch.\n    \"\"\"\n    # Unzip the batch of examples\n    source_sequences, target_sequences = zip(*batch)\n    \n    # Pad sequences to the same length\n    padded_sources = pad_sequence(\n        source_sequences, batch_first=True, \n        padding_value=source_vocab.pad_idx\n    )\n    padded_targets = pad_sequence(\n        target_sequences, batch_first=True, \n        padding_value=target_vocab.pad_idx\n    )\n    \n    # Store original sequence lengths for packed sequence\n    source_lengths = torch.tensor([len(seq) for seq in source_sequences], dtype=torch.long)\n    \n    return padded_sources, source_lengths, padded_targets\n\n\ndef load_transliteration_datasets(\n    language_code='ta',  # Tamil\n    dataset_format='dakshina',\n    data_root=None,\n    batch_size=64,\n    device_name='cpu',\n    num_workers=2,\n    prefetch_factor=4,\n    persistent_workers=True,\n    cache_directory='./cache',\n    use_cached_vocabulary=True\n):\n    \"\"\"\n    Load transliteration datasets for a language.\n    \n    Args:\n        language_code: Language code (e.g., 'ta' for Tamil)\n        dataset_format: Dataset format\n        data_root: Override default dataset path\n        batch_size: Batch size for dataloaders\n        device_name: Device for data loading\n        num_workers: Number of data loading workers\n        prefetch_factor: Batches to prefetch per worker\n        persistent_workers: Keep workers alive between epochs\n        cache_directory: Directory for vocabulary cache\n        use_cached_vocabulary: Whether to use cached vocabulary\n        \n    Returns:\n        tuple: (dataloaders, source_vocab, target_vocab)\n    \"\"\"\n    # Set up data paths\n    if data_root is None:\n        data_root = os.path.join(\n            '/kaggle/working/dakshina_dataset_v1.0',\n            language_code, 'lexicons'\n        )\n    \n    # Set up vocabulary cache\n    if use_cached_vocabulary:\n        os.makedirs(cache_directory, exist_ok=True)\n        vocab_cache_path = os.path.join(\n            cache_directory, f\"{language_code}_{dataset_format}_vocab.pkl\"\n        )\n    \n    # Try to load cached vocabularies\n    if use_cached_vocabulary and os.path.exists(vocab_cache_path):\n        print(f\"Loading cached vocabularies from {vocab_cache_path}\")\n        with open(vocab_cache_path, 'rb') as f:\n            source_vocab, target_vocab = pickle.load(f)\n    else:\n        # Build vocabularies from training and validation data\n        source_texts, target_texts = [], []\n        \n        # Process training and validation data for vocabulary creation\n        for split in ['train', 'dev']:\n            file_path = os.path.join(\n                data_root, f\"{language_code}.translit.sampled.{split}.tsv\"\n            )\n            \n            # Read file in Dakshina format\n            with open(file_path, encoding='utf-8') as f:\n                for line in f:\n                    parts = line.strip().split('\\t')\n                    if len(parts) >= 2:\n                        # In Dakshina: native_script, latin_script\n                        target_texts.append(parts[0])\n                        source_texts.append(parts[1])\n        \n        # Create vocabularies\n        source_vocab = CharacterVocabulary.build_from_text_corpus(source_texts)\n        target_vocab = CharacterVocabulary.build_from_text_corpus(target_texts)\n        \n        # Cache vocabularies\n        if use_cached_vocabulary:\n            with open(vocab_cache_path, 'wb') as f:\n                pickle.dump((source_vocab, target_vocab), f)\n    \n    # Common DataLoader configurations\n    loader_kwargs = {\n        'batch_size': batch_size,\n        'num_workers': num_workers,\n        'prefetch_factor': prefetch_factor,\n        'persistent_workers': persistent_workers and num_workers > 0,\n        'pin_memory': (device_name == 'cuda')\n    }\n    \n    # Create data loaders for each split\n    dataloaders = {}\n    \n    # Map split names to file suffixes\n    split_mapping = {'train': 'train', 'dev': 'dev', 'test': 'test'}\n    \n    for split_name, file_suffix in split_mapping.items():\n        file_path = os.path.join(\n            data_root, f\"{language_code}.translit.sampled.{file_suffix}.tsv\"\n        )\n        \n        # Create dataset\n        dataset = TransliterationDataset(\n            file_path, source_vocab, target_vocab, dataset_type='dakshina'\n        )\n        \n        # Create dataloader\n        dataloaders[split_name] = DataLoader(\n            dataset,\n            shuffle=(split_name == 'train'),  # Only shuffle training data\n            collate_fn=lambda batch: collate_with_padding(batch, source_vocab, target_vocab),\n            **loader_kwargs\n        )\n    \n    return dataloaders, source_vocab, target_vocab\n\n\n# Training and evaluation functions\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm.auto import tqdm\nimport csv\nimport wandb\n\n\ndef calculate_accuracy_metrics(model, dataloader, target_vocab, source_vocab, device):\n    \"\"\"\n    Calculate accuracy and store prediction details.\n    \n    Returns:\n        tuple: (accuracy, correct_predictions, incorrect_predictions)\n    \"\"\"\n    model.eval()\n    total_correct = 0\n    total_examples = 0\n    \n    # Store prediction details for analysis\n    correct_sources = []\n    correct_targets = []\n    correct_predictions = []\n    \n    incorrect_sources = []\n    incorrect_targets = []\n    incorrect_predictions = []\n    \n    with torch.no_grad():\n        for source_batch, source_lengths, target_batch in dataloader:\n            # Move data to device\n            source_batch = source_batch.to(device)\n            source_lengths = source_lengths.to(device)\n            target_batch = target_batch.to(device)\n            \n            # Generate predictions\n            predicted_batch = model.generate_sequence(\n                source_batch, source_lengths, target_vocab, \n                max_length=target_batch.size(1)\n            )\n\n            # Evaluate each example in batch\n            for idx in range(source_batch.size(0)):\n                # Convert to text for comparison\n                predicted_text = target_vocab.convert_indices_to_text(predicted_batch[idx])\n                target_text = target_vocab.convert_indices_to_text(\n                    target_batch[idx, 1:]  # Skip <sos> token\n                )\n                source_text = source_vocab.convert_indices_to_text(source_batch[idx])\n                \n                # Check if prediction matches target\n                is_correct = (predicted_text == target_text)\n                total_correct += int(is_correct)\n                \n                # Store details for analysis\n                if is_correct:\n                    correct_sources.append(source_text)\n                    correct_targets.append(target_text)\n                    correct_predictions.append(predicted_text)\n                else:\n                    incorrect_sources.append(source_text)\n                    incorrect_targets.append(target_text)\n                    incorrect_predictions.append(predicted_text)\n                \n            total_examples += source_batch.size(0)\n\n    # Calculate overall accuracy\n    accuracy = total_correct / total_examples if total_examples > 0 else 0.0\n    \n    return (\n        accuracy, \n        (correct_sources, correct_targets, correct_predictions),\n        (incorrect_sources, incorrect_targets, incorrect_predictions)\n    )\n\n\ndef save_predictions_to_file(sources, targets, predictions, output_path):\n    \"\"\"Save prediction details to CSV file for analysis.\"\"\"\n    with open(output_path, mode='w', newline='', encoding='utf-8') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Source', 'Target', 'Prediction'])\n        writer.writerows(zip(sources, targets, predictions))\n    \n    return output_path\n\n\ndef train_transliteration_model(\n    model, \n    dataloaders, \n    source_vocab, \n    target_vocab, \n    device,\n    config,\n    model_save_path=None,\n    enable_wandb_logging=True\n):\n    \"\"\"\n    Train transliteration model.\n    \n    Args:\n        model: Transliteration model\n        dataloaders: Dictionary of data loaders\n        source_vocab: Source vocabulary\n        target_vocab: Target vocabulary\n        device: Device to use\n        config: Training configuration\n        model_save_path: Path to save best model\n        enable_wandb_logging: Whether to log to W&B\n        \n    Returns:\n        tuple: (trained_model, test_accuracy)\n    \"\"\"\n    # Define loss function\n    loss_function = nn.CrossEntropyLoss(ignore_index=target_vocab.pad_idx)\n    \n    # Configure optimizer\n    if config.optimizer.lower() == 'adam':\n        optimizer = optim.Adam(model.parameters(), lr=config.lr)\n    elif config.optimizer.lower() == 'nadam':\n        optimizer = optim.NAdam(model.parameters(), lr=config.lr)\n    else:\n        optimizer = optim.Adam(model.parameters(), lr=config.lr)\n    \n    # Track best validation performance\n    best_validation_accuracy = 0.0\n    \n    # Training loop\n    for epoch in tqdm(range(1, config.epochs + 1), desc=\"Epochs\", position=0):\n        # Training phase\n        model.train()\n        epoch_loss = 0.0\n\n        # Process training batches\n        train_loader = tqdm(\n            dataloaders['train'], desc=f\"Training epoch {epoch}\", \n            leave=False, position=1\n        )\n        \n        for source_batch, source_lengths, target_batch in train_loader:\n            # Move data to device\n            source_batch = source_batch.to(device)\n            source_lengths = source_lengths.to(device)\n            target_batch = target_batch.to(device)\n\n            # Forward pass with teacher forcing\n            optimizer.zero_grad()\n            output_logits = model(\n                source_batch, source_lengths, target_batch, \n                teacher_forcing_prob=config.teacher_forcing\n            )\n            \n            # Calculate loss\n            # Reshape outputs and targets to match cross-entropy requirements\n            loss = loss_function(\n                output_logits.reshape(-1, output_logits.size(-1)),\n                target_batch[:, 1:].reshape(-1)  # Skip <sos> token\n            )\n            \n            # Backward pass and optimize\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n            optimizer.step()\n            \n            epoch_loss += loss.item()\n        \n        train_loader.close()\n        average_train_loss = epoch_loss / len(dataloaders['train'])\n\n        # Validation phase\n        validation_loss = 0.0\n        validation_loader = tqdm(\n            dataloaders['dev'], desc=f\"Validation epoch {epoch}\", \n            leave=False, position=1\n        )\n        \n        model.eval()\n        with torch.no_grad():\n            for source_batch, source_lengths, target_batch in validation_loader:\n                # Move data to device\n                source_batch = source_batch.to(device)\n                source_lengths = source_lengths.to(device)\n                target_batch = target_batch.to(device)\n                \n                # Forward pass without teacher forcing\n                output_logits = model(\n                    source_batch, source_lengths, target_batch, \n                    teacher_forcing_prob=0.0\n                )\n                \n                # Calculate loss\n                loss = loss_function(\n                    output_logits.reshape(-1, output_logits.size(-1)),\n                    target_batch[:, 1:].reshape(-1)\n                )\n                \n                validation_loss += loss.item()\n        \n        validation_loader.close()\n        average_validation_loss = validation_loss / len(dataloaders['dev'])\n\n        # Calculate accuracy metrics\n        train_results = calculate_accuracy_metrics(\n            model, dataloaders['train'], target_vocab, source_vocab, device\n        )\n        train_accuracy = train_results[0]\n        \n        validation_results = calculate_accuracy_metrics(\n            model, dataloaders['dev'], target_vocab, source_vocab, device\n        )\n        validation_accuracy = validation_results[0]\n        \n        # Save best model\n        if validation_accuracy > best_validation_accuracy and model_save_path:\n            best_validation_accuracy = validation_accuracy\n            torch.save(model.state_dict(), model_save_path)\n            print(f\"Saved new best model with validation accuracy: {validation_accuracy:.4f}\")\n            \n            # Save prediction analysis periodically\n            if epoch == config.epochs or epoch % 5 == 0:\n                # Save correct predictions\n                correct_data = validation_results[1]\n                save_predictions_to_file(\n                    correct_data[0], correct_data[1], correct_data[2],\n                    f\"correct_predictions_epoch_{epoch}.csv\"\n                )\n                \n                # Save incorrect predictions\n                incorrect_data = validation_results[2]\n                save_predictions_to_file(\n                    incorrect_data[0], incorrect_data[1], incorrect_data[2],\n                    f\"incorrect_predictions_epoch_{epoch}.csv\"\n                )\n\n        # Log metrics\n        print(f\"Epoch {epoch}/{config.epochs}:\")\n        print(f\"  Train Loss: {average_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n        print(f\"  Validation Loss: {average_validation_loss:.4f}, Validation Accuracy: {validation_accuracy:.4f}\")\n        \n        # Log to W&B if enabled\n        if enable_wandb_logging:\n            wandb.log({\n                'epoch': epoch,\n                'train_loss': average_train_loss,\n                'validation_loss': average_validation_loss,\n                'train_accuracy': train_accuracy,\n                'validation_accuracy': validation_accuracy\n            })\n\n    # For simplicity, we skip test set evaluation here\n    test_accuracy = 0\n    \n    return model, test_accuracy\n\n\n# Hyperparameter sweep configuration\nimport wandb\nimport torch\nfrom tqdm.auto import tqdm\nimport os\nimport random\nimport numpy as np\n\n\ndef run_model_with_config():\n    \"\"\"Run a model training with the current W&B configuration.\"\"\"\n    # Initialize W&B run\n    run = wandb.init()\n    config = run.config\n    \n    # Set seeds for reproducibility\n    set_random_seeds(config.seed if hasattr(config, 'seed') else 42)\n    \n    # Set device\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    print(f\"Using device: {device}\")\n    \n    # Create a unique run name based on configuration\n    run_name = (\n        f\"{config.cell_type}_{config.encoder_layers}l_{config.embedding_dim}e_\"\n        f\"{config.hidden_dim}h_{'bid' if config.bidirectional else 'uni'}_\"\n        f\"{config.dropout}d_{config.teacher_forcing}tf_{config.optimizer}\"\n    )\n    wandb.run.name = run_name\n    \n    # Load datasets\n    dataloaders, source_vocab, target_vocab = load_transliteration_datasets(\n        language_code='ta',  # Tamil\n        batch_size=config.batch_size,\n        device_name=device\n    )\n    \n    # Create encoder\n    encoder = SourceEncoder(\n        vocabulary_size=source_vocab.size,\n        embedding_dim=config.embedding_dim,\n        hidden_dim=config.hidden_dim,\n        num_layers=config.encoder_layers,\n        rnn_type=config.cell_type,\n        dropout_rate=config.dropout,\n        use_bidirectional=config.bidirectional\n    ).to(device)\n    \n    # Calculate encoder output dimension\n    encoder_output_dim = config.hidden_dim * 2 if config.bidirectional else config.hidden_dim\n    \n    # Create decoder\n    decoder = TargetDecoder(\n        target_vocab_size=target_vocab.size,\n        embedding_dim=config.embedding_dim,\n        encoder_dim=encoder_output_dim,\n        decoder_dim=config.hidden_dim,\n        num_layers=config.encoder_layers,\n        rnn_type=config.cell_type,\n        dropout_rate=config.dropout\n    ).to(device)\n    \n    # Create full model\n    model = TransliterationModel(\n        encoder, decoder, \n        padding_idx=source_vocab.pad_idx, \n        device_name=device\n    ).to(device)\n    \n    # Train model\n    best_model_path = f\"model_{run_name}.pt\"\n    _, test_accuracy = train_transliteration_model(\n        model=model,\n        dataloaders=dataloaders,\n        source_vocab=source_vocab,\n        target_vocab=target_vocab,\n        device=device,\n        config=config,\n        model_save_path=best_model_path,\n        enable_wandb_logging=True\n    )\n    \n    # Finish the run\n    wandb.finish()\n\n\n# Main script to run sweep\nif __name__ == \"__main__\":\n    # Define sweep configuration\n    sweep_cfg = {\n        \n        'method': 'bayes',  # Use Bayesian optimization\n        'name':'Transliteration_without_Attention',\n        'metric': {'name': 'val_acc', 'goal': 'maximize'},\n        'parameters': {\n            \n            # Model architecture\n            'emb_size': {'values': [32,64,128, 256, 512]},\n            'hidden_size': {'values': [32,64,128, 256, 512, 1024]},\n            'enc_layers': {'values': [1, 2, 3, 4]},\n            'cell': {'values': ['RNN', 'GRU', 'LSTM']},  \n            'bidirectional': {'values': [True, False]},  # Bidirectional encode\n            \n            # Training parameters\n            'dropout': {'values': [0.0, 0.1, 0.2, 0.3, 0.5]},\n            'lr': {'values': [1e-4, 2e-4, 5e-4, 8e-4, 1e-3]},\n            'batch_size': {'values': [32, 64, 128]},\n            'epochs': {'values': [10, 15, 20]},\n            'teacher_forcing': {'values': [0.3, 0.5, 0.7, 1.0]},  # Explicit teacher forcing\n            'optimizer': {'values': ['Adam', 'NAdam']},  # Added optimizer options\n            # Reproducibility\n            'seed': {'values': [42, 43, 44, 45, 46]},  # Different seeds for robustness\n        }\n    }\n\n    # Initialize and run sweep\n    sweep_id = wandb.sweep(\n        sweep_config,\n        project='DA6401_A3'\n    )\n    \n    # Run sweep agent\n    wandb.agent(sweep_id, function=run_model_with_config, count=30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T05:30:45.582729Z","iopub.execute_input":"2025-05-20T05:30:45.583286Z","execution_failed":"2025-05-20T09:55:36.648Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">GRU_3l_128e_256h_bid_0.3d_0.3tf_NAdam_s42</strong> at: <a href='https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3/runs/xboinabb' target=\"_blank\">https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3/runs/xboinabb</a><br> View project at: <a href='https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_052417-xboinabb/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_053045-5nl5rzfj</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3/runs/5nl5rzfj' target=\"_blank\">GRU_3l_128e_256h_bid_0.3d_0.3tf_NAdam_s42</a></strong> to <a href='https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3/runs/5nl5rzfj' target=\"_blank\">https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3/runs/5nl5rzfj</a>"},"metadata":{}},{"name":"stdout","text":"Using device: cuda\nLoading cached vocabularies from ./cache/te_dakshina_vocab.pkl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epochs:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03da77ee9b594ee6a1e17c21aa6fa819"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Train 1:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 1:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Saved new best model with validation accuracy: 0.3924\nEpoch 1/10:\n  Train Loss: 1.1262, Train Acc: 0.4864\n  Val Loss: 0.7815, Val Acc: 0.3924\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 2:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 2:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Saved new best model with validation accuracy: 0.4000\nEpoch 2/10:\n  Train Loss: 0.5373, Train Acc: 0.4988\n  Val Loss: 0.7040, Val Acc: 0.4000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 3:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 3:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Saved new best model with validation accuracy: 0.4695\nEpoch 3/10:\n  Train Loss: 0.4352, Train Acc: 0.6503\n  Val Loss: 0.6903, Val Acc: 0.4695\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 4:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 4:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 4/10:\n  Train Loss: 0.3838, Train Acc: 0.6760\n  Val Loss: 0.6739, Val Acc: 0.4695\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 5:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 5:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 5/10:\n  Train Loss: 0.3589, Train Acc: 0.6470\n  Val Loss: 0.6958, Val Acc: 0.4491\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 6:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 6:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Saved new best model with validation accuracy: 0.4901\nEpoch 6/10:\n  Train Loss: 0.3403, Train Acc: 0.7171\n  Val Loss: 0.6802, Val Acc: 0.4901\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 7:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 7:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 7/10:\n  Train Loss: 0.3299, Train Acc: 0.6936\n  Val Loss: 0.6989, Val Acc: 0.4705\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 8:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 8:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Saved new best model with validation accuracy: 0.4918\nEpoch 8/10:\n  Train Loss: 0.3206, Train Acc: 0.7254\n  Val Loss: 0.6857, Val Acc: 0.4918\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 9:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 9:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 9/10:\n  Train Loss: 0.3195, Train Acc: 0.6401\n  Val Loss: 0.6949, Val Acc: 0.4552\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 10:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 10:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 10/10:\n  Train Loss: 0.3170, Train Acc: 0.7376\n  Val Loss: 0.7352, Val Acc: 0.4760\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>▁▁▆▆▅▇▇█▅█</td></tr><tr><td>train_loss</td><td>█▃▂▂▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▂▆▆▅█▇█▅▇</td></tr><tr><td>val_loss</td><td>█▃▂▁▂▁▃▂▂▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_acc</td><td>0.73763</td></tr><tr><td>train_loss</td><td>0.31698</td></tr><tr><td>val_acc</td><td>0.47598</td></tr><tr><td>val_loss</td><td>0.73522</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">GRU_3l_128e_256h_bid_0.3d_0.3tf_NAdam_s42</strong> at: <a href='https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3/runs/5nl5rzfj' target=\"_blank\">https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3/runs/5nl5rzfj</a><br> View project at: <a href='https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_053045-5nl5rzfj/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_054759-0ziuyi25</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3/runs/0ziuyi25' target=\"_blank\">LSTM_1l_256e_512h_bid_0.5d_0.5tf_NAdam_s44</a></strong> to <a href='https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3/runs/0ziuyi25' target=\"_blank\">https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3/runs/0ziuyi25</a>"},"metadata":{}},{"name":"stdout","text":"Using device: cuda\nLoading cached vocabularies from ./cache/te_dakshina_vocab.pkl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epochs:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41c65bb966e54309abe2d47f5c5f8cfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Train 1:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32657159f1044f31af9f50d33858940e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 1:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"291ee3f210cc4ab5bb91cf0e243459de"}},"metadata":{}},{"name":"stdout","text":"Saved new best model with validation accuracy: 0.3924\nEpoch 1/10:\n  Train Loss: 0.8821, Train Acc: 0.4717\n  Val Loss: 0.7612, Val Acc: 0.3924\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 2:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6ea7df03b95416d9776b4a0ef89f737"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 2:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2da0186d60784c468dd5343510d4e984"}},"metadata":{}},{"name":"stdout","text":"Epoch 2/10:\n  Train Loss: 0.3274, Train Acc: 0.4733\n  Val Loss: 0.6899, Val Acc: 0.3621\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 3:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e17308d289b74ceba81433ab6096511b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 3:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18266da3f24e43549672cbc7d7ba314c"}},"metadata":{}},{"name":"stdout","text":"Saved new best model with validation accuracy: 0.3970\nEpoch 3/10:\n  Train Loss: 0.2234, Train Acc: 0.5558\n  Val Loss: 0.7137, Val Acc: 0.3970\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 4:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d675adca30d47e2a9d8bb8735f6f554"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 4:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7d9bc5db3e841d090ab930ebf199746"}},"metadata":{}},{"name":"stdout","text":"Saved new best model with validation accuracy: 0.4198\nEpoch 4/10:\n  Train Loss: 0.1635, Train Acc: 0.5828\n  Val Loss: 0.7027, Val Acc: 0.4198\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 5:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 5:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 5/10:\n  Train Loss: 0.1273, Train Acc: 0.3911\n  Val Loss: 0.7471, Val Acc: 0.3097\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 6:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 6:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 6/10:\n  Train Loss: 0.0999, Train Acc: 0.5882\n  Val Loss: 0.7745, Val Acc: 0.3817\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 7:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 7:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 7/10:\n  Train Loss: 0.0814, Train Acc: 0.5473\n  Val Loss: 0.7889, Val Acc: 0.3613\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 8:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 8:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 8/10:\n  Train Loss: 0.0717, Train Acc: 0.5645\n  Val Loss: 0.8248, Val Acc: 0.3616\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 9:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 9:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Saved new best model with validation accuracy: 0.4394\nEpoch 9/10:\n  Train Loss: 0.0616, Train Acc: 0.7116\n  Val Loss: 0.8680, Val Acc: 0.4394\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 10:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 10:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Saved new best model with validation accuracy: 0.4531\nEpoch 10/10:\n  Train Loss: 0.0554, Train Acc: 0.7633\n  Val Loss: 0.8919, Val Acc: 0.4531\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>▃▃▄▅▁▅▄▄▇█</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▅▄▅▆▁▅▄▄▇█</td></tr><tr><td>val_loss</td><td>▃▁▂▁▃▄▄▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_acc</td><td>0.76331</td></tr><tr><td>train_loss</td><td>0.05543</td></tr><tr><td>val_acc</td><td>0.45311</td></tr><tr><td>val_loss</td><td>0.89188</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">LSTM_1l_256e_512h_bid_0.5d_0.5tf_NAdam_s44</strong> at: <a href='https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3/runs/0ziuyi25' target=\"_blank\">https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3/runs/0ziuyi25</a><br> View project at: <a href='https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_054759-0ziuyi25/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_060132-own39f9v</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3/runs/own39f9v' target=\"_blank\">GRU_4l_256e_1024h_uni_0.0d_0.7tf_Adam_s46</a></strong> to <a href='https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3/runs/own39f9v' target=\"_blank\">https://wandb.ai/surendarmohan283-indian-institute-of-technology-madras/DA6401_A3/runs/own39f9v</a>"},"metadata":{}},{"name":"stdout","text":"Using device: cuda\nLoading cached vocabularies from ./cache/te_dakshina_vocab.pkl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epochs:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2fd49d61a534e1faa4744b92a767f70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Train 1:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 1:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Saved new best model with validation accuracy: 0.2958\nEpoch 1/20:\n  Train Loss: 0.7627, Train Acc: 0.3480\n  Val Loss: 0.8287, Val Acc: 0.2958\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 2:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 2:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 2/20:\n  Train Loss: 0.2182, Train Acc: 0.2371\n  Val Loss: 0.7490, Val Acc: 0.1943\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 3:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 3:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 3/20:\n  Train Loss: 0.1369, Train Acc: 0.3998\n  Val Loss: 0.7798, Val Acc: 0.2736\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 4:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 4:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 4/20:\n  Train Loss: 0.0935, Train Acc: 0.3428\n  Val Loss: 0.8113, Val Acc: 0.2395\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 5:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 5:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 5/20:\n  Train Loss: 0.0697, Train Acc: 0.3405\n  Val Loss: 0.8425, Val Acc: 0.2515\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 6:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 6:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 6/20:\n  Train Loss: 0.0575, Train Acc: 0.3145\n  Val Loss: 0.8661, Val Acc: 0.2277\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 7:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 7:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 7/20:\n  Train Loss: 0.0490, Train Acc: 0.3029\n  Val Loss: 0.9856, Val Acc: 0.2259\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 8:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 8:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Saved new best model with validation accuracy: 0.3188\nEpoch 8/20:\n  Train Loss: 0.0430, Train Acc: 0.4541\n  Val Loss: 0.9369, Val Acc: 0.3188\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 9:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 9:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Saved new best model with validation accuracy: 0.4425\nEpoch 9/20:\n  Train Loss: 0.0390, Train Acc: 0.6670\n  Val Loss: 0.9226, Val Acc: 0.4425\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 10:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 10:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 10/20:\n  Train Loss: 0.0343, Train Acc: 0.4532\n  Val Loss: 0.9878, Val Acc: 0.2898\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 11:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val 11:   0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 11/20:\n  Train Loss: 0.0336, Train Acc: 0.4677\n  Val Loss: 1.0406, Val Acc: 0.2967\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 12:   0%|          | 0/1830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d26d2984cbb4ff891f511821e985ed2"}},"metadata":{}}],"execution_count":null}]}