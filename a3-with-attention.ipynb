{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Downloading dakshina dataset\n!yes | wget \"https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:09:40.116109Z","iopub.execute_input":"2025-05-20T19:09:40.116421Z","iopub.status.idle":"2025-05-20T19:09:46.885782Z","shell.execute_reply.started":"2025-05-20T19:09:40.116397Z","shell.execute_reply":"2025-05-20T19:09:46.884729Z"}},"outputs":[{"name":"stdout","text":"--2025-05-20 19:09:40--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\nResolving storage.googleapis.com (storage.googleapis.com)... 173.194.216.207, 173.194.217.207, 173.194.215.207, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|173.194.216.207|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2008340480 (1.9G) [application/x-tar]\nSaving to: ‘dakshina_dataset_v1.0.tar’\n\ndakshina_dataset_v1 100%[===================>]   1.87G   316MB/s    in 6.5s    \n\n2025-05-20 19:09:46 (296 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n\nyes: standard output: Broken pipe\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Unzipping dataset\n!yes | tar xopf dakshina_dataset_v1.0.tar","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:09:55.851745Z","iopub.execute_input":"2025-05-20T19:09:55.852079Z","iopub.status.idle":"2025-05-20T19:09:59.064377Z","shell.execute_reply.started":"2025-05-20T19:09:55.852049Z","shell.execute_reply":"2025-05-20T19:09:59.063366Z"}},"outputs":[{"name":"stdout","text":"yes: standard output: Broken pipe\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:10:29.861023Z","iopub.execute_input":"2025-05-20T19:10:29.861364Z","iopub.status.idle":"2025-05-20T19:10:33.021552Z","shell.execute_reply.started":"2025-05-20T19:10:29.861338Z","shell.execute_reply":"2025-05-20T19:10:33.020714Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"wandb.login(key='49f8f505158ee3693f0cacf0a82118bd4e636e8c')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:10:35.330829Z","iopub.execute_input":"2025-05-20T19:10:35.331261Z","iopub.status.idle":"2025-05-20T19:10:41.665660Z","shell.execute_reply.started":"2025-05-20T19:10:35.331235Z","shell.execute_reply":"2025-05-20T19:10:41.664783Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msurendarmohan283\u001b[0m (\u001b[33msurendarmohan283-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nimport random\n\n# For reproducibility - keep the same for consistency\ndef initialize_random_seeds(seed=42):\n    \"\"\"Configure all random seeds for reproducibility\"\"\"\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nclass EncoderNetwork(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, rnn_type='LSTM', \n                 dropout_rate=0.0, use_bidirectional=False):\n        super().__init__()\n        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.use_bidirectional = use_bidirectional\n        self.rnn_type = rnn_type\n        self.num_layers = num_layers\n        self.hidden_dim = hidden_dim\n        \n        # Adjust output dimension for bidirectional\n        self.output_dim = hidden_dim * 2 if use_bidirectional else hidden_dim\n        \n        # Select RNN type\n        rnn_classes = {'LSTM': nn.LSTM, 'GRU': nn.GRU, 'RNN': nn.RNN}\n        selected_rnn = rnn_classes[rnn_type]\n        \n        self.rnn_layer = selected_rnn(\n            embedding_dim,\n            hidden_dim,\n            num_layers=num_layers,\n            dropout=dropout_rate if num_layers > 1 else 0.0,\n            batch_first=True,\n            bidirectional=use_bidirectional\n        )\n\n    def forward(self, input_sequence, sequence_lengths):\n        # input_sequence: [Batch, SeqLen], sequence_lengths: [Batch]\n        embedded_sequence = self.token_embedding(input_sequence)  # [Batch, SeqLen, EmbeddingDim]\n        \n        # Pack to handle variable lengths efficiently\n        packed_embeddings = pack_padded_sequence(\n            embedded_sequence, \n            sequence_lengths.cpu(), \n            batch_first=True, \n            enforce_sorted=False\n        )\n        \n        # Process through RNN\n        packed_outputs, hidden_states = self.rnn_layer(packed_embeddings)\n        \n        # Unpack to get full sequence\n        outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True)  # [Batch, SeqLen, HiddenDim*Directions]\n        \n        # Process bidirectional states\n        if self.use_bidirectional:\n            if self.rnn_type == 'LSTM':\n                # Handle LSTM's (hidden, cell) pair\n                h_states, c_states = hidden_states\n                # Combine forward and backward directions\n                h_combined = torch.add(h_states[0:self.num_layers], h_states[self.num_layers:]) / 2\n                c_combined = torch.add(c_states[0:self.num_layers], c_states[self.num_layers:]) / 2\n                hidden_states = (h_combined, c_combined)\n            else:\n                # For GRU/RNN with only hidden state\n                hidden_combined = torch.add(hidden_states[0:self.num_layers], hidden_states[self.num_layers:]) / 2\n                hidden_states = hidden_combined\n                \n        return outputs, hidden_states\n\n\nclass AttentionMechanism(nn.Module):\n    def __init__(self, encoder_dim, decoder_dim):\n        super().__init__()\n        self.attention_projection = nn.Linear(encoder_dim + decoder_dim, decoder_dim)\n        self.energy_vector = nn.Linear(decoder_dim, 1, bias=False)\n\n    def forward(self, decoder_hidden, encoder_outputs, attention_mask):\n        # decoder_hidden: [Batch, HiddenDim], encoder_outputs: [Batch, SrcLen, HiddenDim], \n        # attention_mask: [Batch, SrcLen]\n        batch_size, src_len, hidden_dim = encoder_outputs.size()\n        \n        # Expand decoder hidden state to match encoder outputs\n        expanded_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)  # [Batch, SrcLen, HiddenDim]\n        \n        # Calculate attention scores\n        energy = torch.tanh(self.attention_projection(\n            torch.cat((expanded_hidden, encoder_outputs), dim=2)\n        ))  # [Batch, SrcLen, HiddenDim]\n        \n        attention_scores = self.energy_vector(energy).squeeze(2)  # [Batch, SrcLen]\n        \n        # Apply mask to prevent attention to padding tokens\n        attention_scores = attention_scores.masked_fill(~attention_mask, -1e9)\n        \n        # Apply softmax to get attention weights\n        return torch.softmax(attention_scores, dim=1)  # [Batch, SrcLen]\n\n\nclass DecoderNetwork(nn.Module):\n    \"\"\"\n    Decoder with two operating modes:\n        • with_attention=True  – Uses Bahdanau attention mechanism (default)\n        • with_attention=False – Uses plain RNN decoder without attention\n\n    Always returns (logits, hidden_state, attention_weights_or_None)\n    \"\"\"\n    def __init__(self, vocab_size, embedding_dim, encoder_dim, decoder_dim,\n                 num_layers=1, rnn_type=\"LSTM\", dropout_rate=0.0, with_attention=True):\n        super().__init__()\n        self.with_attention = with_attention\n        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.rnn_type = rnn_type\n\n        # Configure input dimensions based on attention usage\n        if with_attention:\n            self.attention = AttentionMechanism(encoder_dim, decoder_dim)\n            rnn_input_size = embedding_dim + encoder_dim  # Concatenate embedding with context\n            output_projection_size = decoder_dim + encoder_dim + embedding_dim  # Full projection\n        else:\n            rnn_input_size = embedding_dim\n            output_projection_size = decoder_dim + embedding_dim\n\n        # Select RNN type\n        rnn_classes = {\"LSTM\": nn.LSTM, \"GRU\": nn.GRU, \"RNN\": nn.RNN}\n        selected_rnn = rnn_classes[rnn_type]\n        \n        self.rnn_layer = selected_rnn(\n            rnn_input_size, \n            decoder_dim,\n            num_layers=num_layers,\n            dropout=dropout_rate if num_layers > 1 else 0.0,\n            batch_first=True\n        )\n        \n        self.output_projection = nn.Linear(output_projection_size, vocab_size)\n\n    def forward(self, current_token, hidden_state, encoder_outputs, attention_mask):\n        \"\"\"\n        Process a single decoding step\n        \n        Args:\n            current_token: [Batch] - Current input tokens\n            hidden_state: tuple|tensor - Previous decoder state\n            encoder_outputs: [Batch, SrcLen, EncoderDim] - All encoder outputs\n            attention_mask: [Batch, SrcLen] - Mask for valid source positions\n        \"\"\"\n        # Embed the current token\n        token_embedding = self.token_embedding(current_token).unsqueeze(1)  # [Batch, 1, EmbeddingDim]\n\n        if self.with_attention:\n            # Extract the relevant hidden state for attention\n            if self.rnn_type == 'LSTM':\n                decoder_hidden = hidden_state[0][-1]\n            else:\n                decoder_hidden = hidden_state[-1]\n                \n            # Calculate attention weights and context vector\n            attention_weights = self.attention(decoder_hidden, encoder_outputs, attention_mask)  # [Batch, SrcLen]\n            context_vector = torch.bmm(\n                attention_weights.unsqueeze(1), \n                encoder_outputs\n            )  # [Batch, 1, EncoderDim]\n            \n            # Concatenate embedding with context for RNN input\n            rnn_input = torch.cat((token_embedding, context_vector), dim=2)  # [Batch, 1, EmbeddingDim+EncoderDim]\n        else:\n            context_vector = None\n            attention_weights = None\n            rnn_input = token_embedding  # [Batch, 1, EmbeddingDim]\n\n        # Process through RNN\n        output, new_hidden = self.rnn_layer(rnn_input, hidden_state)  # [Batch, 1, DecoderDim]\n        \n        # Remove sequence dimension\n        output = output.squeeze(1)  # [Batch, DecoderDim]\n        embedding = token_embedding.squeeze(1)  # [Batch, EmbeddingDim]\n\n        # Create final projection input\n        if self.with_attention:\n            context = context_vector.squeeze(1)  # [Batch, EncoderDim]\n            logits = self.output_projection(torch.cat((output, context, embedding), dim=1))\n        else:\n            logits = self.output_projection(torch.cat((output, embedding), dim=1))\n\n        return logits, new_hidden, attention_weights\n\n\nclass Seq2SeqModel(nn.Module):\n    def __init__(self, encoder, decoder, padding_idx, device='cpu'):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.padding_idx = padding_idx\n        self.device = device\n\n    def forward(self, src_sequence, src_lengths, tgt_sequence, teacher_forcing_ratio=0.5):\n        \"\"\"\n        Training forward pass with teacher forcing\n        \n        Args:\n            src_sequence: Input sequence [Batch, SrcLen]\n            src_lengths: Lengths of source sequences [Batch]\n            tgt_sequence: Target sequence [Batch, TgtLen]\n            teacher_forcing_ratio: Probability of using teacher forcing\n        \"\"\"\n        # Encode source sequence\n        encoder_outputs, hidden_state = self.encoder(src_sequence, src_lengths)\n        \n        # Create mask for attention (1 for real tokens, 0 for padding)\n        attention_mask = (src_sequence != self.padding_idx)\n        \n        # Initialize for decoding\n        batch_size, target_length = tgt_sequence.size()\n        output_vocab_size = self.decoder.output_projection.out_features\n        outputs = torch.zeros(batch_size, target_length-1, output_vocab_size, device=self.device)\n        \n        # Initialize with start token\n        current_token = tgt_sequence[:, 0]  # <sos>\n        \n        # Decode one step at a time\n        for t in range(1, target_length):\n            # Compute output for current step\n            step_output, hidden_state, _ = self.decoder(\n                current_token, hidden_state, encoder_outputs, attention_mask\n            )\n            outputs[:, t-1] = step_output\n            \n            # Decide whether to use teacher forcing\n            use_teacher_forcing = random.random() < teacher_forcing_ratio\n            \n            if use_teacher_forcing:\n                # Use ground truth as next input\n                current_token = tgt_sequence[:, t]\n            else:\n                # Use model prediction as next input\n                current_token = step_output.argmax(1)\n                \n        return outputs\n\n    def generate_greedy(self, src_sequence, src_lengths, target_vocab, max_length=50):\n        \"\"\"\n        Generate output sequence using greedy decoding\n        \"\"\"\n        # Encode source sequence\n        encoder_outputs, hidden_state = self.encoder(src_sequence, src_lengths)\n        attention_mask = (src_sequence != self.padding_idx)\n        \n        # Initialize decoding with start token\n        batch_size = src_sequence.size(0)\n        current_token = torch.full(\n            (batch_size,), \n            target_vocab.sos_idx, \n            device=self.device, \n            dtype=torch.long\n        )\n        \n        generated_tokens = []\n        \n        # Generate tokens step by step\n        for _ in range(max_length):\n            step_output, hidden_state, _ = self.decoder(\n                current_token, hidden_state, encoder_outputs, attention_mask\n            )\n            \n            # Select most likely token\n            current_token = step_output.argmax(1)\n            generated_tokens.append(current_token.unsqueeze(1))\n            \n            # Stop if all sequences have generated end token\n            if (current_token == target_vocab.eos_idx).all():\n                break\n                \n        return torch.cat(generated_tokens, dim=1)\n\n\n# Download Dakshina dataset - use for setup but no changes here\n# !yes | wget \"https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\"\n# !yes | tar xopf dakshina_dataset_v1.0.tar\n\n# Vocabulary handling class\nclass CharacterVocabulary:\n    \"\"\"Character-level vocabulary for transliteration tasks\"\"\"\n    def __init__(self, token_list=None, special_tokens=['<pad>','<sos>','<eos>','<unk>']):\n        self.special_tokens = special_tokens\n        self.idx2token = list(special_tokens) + (token_list or [])\n        self.token2idx = {token:idx for idx, token in enumerate(self.idx2token)}\n\n    @classmethod\n    def create_from_text_collection(cls, text_collection):\n        \"\"\"Build vocabulary from a collection of texts\"\"\"\n        unique_chars = sorted({char for text in text_collection for char in text})\n        return cls(token_list=unique_chars)\n    \n    @classmethod\n    def create_from_data_file(cls, file_path, source_col='src', target_col='trg', is_csv_format=True):\n        \"\"\"\n        Build vocabulary from a data file\n        \"\"\"\n        if is_csv_format:\n            import pandas as pd\n            df = pd.read_csv(file_path, header=None, names=[source_col, target_col])\n            all_texts = df[source_col].dropna().tolist() + df[target_col].dropna().tolist()\n        else:\n            all_texts = []\n            with open(file_path, encoding='utf-8') as f:\n                for line in f:\n                    parts = line.strip().split('\\t')\n                    if len(parts) >= 2:\n                        all_texts.extend([parts[0], parts[1]])\n        \n        return cls.create_from_text_collection(all_texts)\n\n    def save_to_json(self, path):\n        \"\"\"Save vocabulary to a JSON file\"\"\"\n        import json\n        with open(path, 'w', encoding='utf-8') as f:\n            json.dump(self.idx2token, f, ensure_ascii=False)\n\n    @classmethod\n    def load_from_json(cls, path):\n        \"\"\"Load vocabulary from a JSON file\"\"\"\n        import json\n        with open(path, encoding='utf-8') as f:\n            idx2token = json.load(f)\n        \n        vocab = cls(token_list=[])\n        vocab.idx2token = idx2token\n        vocab.token2idx = {token:idx for idx, token in enumerate(idx2token)}\n        return vocab\n\n    def encode_text(self, text, add_start=False, add_end=False):\n        \"\"\"Convert text to token indices\"\"\"\n        indices = []\n        if add_start: \n            indices.append(self.token2idx['<sos>'])\n            \n        for char in text:\n            indices.append(self.token2idx.get(char, self.token2idx['<unk>']))\n            \n        if add_end: \n            indices.append(self.token2idx['<eos>'])\n            \n        return indices\n\n    def decode_indices(self, indices, remove_special=True, join_chars=True):\n        \"\"\"Convert token indices back to text\"\"\"\n        # Handle tensor input\n        if hasattr(indices, 'tolist'):\n            indices = indices.tolist()\n            \n        # Convert indices to characters\n        chars = [self.idx2token[idx] for idx in indices if idx < len(self.idx2token)]\n        \n        # Remove special tokens if requested\n        if remove_special:\n            chars = [c for c in chars if c not in self.special_tokens]\n            \n        # Return as string or character list\n        return ''.join(chars) if join_chars else chars\n    \n    def batch_decode_indices(self, batch_indices, remove_special=True):\n        \"\"\"Decode a batch of index sequences\"\"\"\n        return [self.decode_indices(seq, remove_special=remove_special) for seq in batch_indices]\n    \n    def get_statistics(self):\n        \"\"\"Get vocabulary statistics\"\"\"\n        return {\n            'total_size': len(self.idx2token),\n            'special_tokens': len(self.special_tokens),\n            'character_count': len(self.idx2token) - len(self.special_tokens)\n        }\n    \n    def __len__(self):\n        return len(self.idx2token)\n\n    @property\n    def pad_idx(self): return self.token2idx['<pad>']\n    \n    @property\n    def sos_idx(self): return self.token2idx['<sos>']\n    \n    @property\n    def eos_idx(self): return self.token2idx['<eos>']\n    \n    @property\n    def unk_idx(self): return self.token2idx['<unk>']\n    \n    @property\n    def size(self): return len(self.idx2token)\n\n\n# Data loading utilities\nimport os\nimport torch\nimport pickle\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nimport pandas as pd\n\nclass TransliterationDataset(Dataset):\n    \"\"\"Dataset for transliteration tasks\"\"\"\n    \n    def __init__(self, data_path, source_vocab, target_vocab, dataset_format='dakshina'):\n        \"\"\"\n        Initialize dataset\n        \n        Args:\n            data_path: Path to data file\n            source_vocab: Source language vocabulary\n            target_vocab: Target language vocabulary\n            dataset_format: Format specification ('dakshina')\n        \"\"\"\n        self.examples = []\n        self.dataset_format = dataset_format\n        \n        if dataset_format == 'dakshina':\n            # Process tab-separated format\n            for src_text, tgt_text in read_transliteration_pairs(data_path):\n                # Encode sequences with start/end tokens\n                src_indices = source_vocab.encode_text(src_text, add_start=True, add_end=True)\n                tgt_indices = target_vocab.encode_text(tgt_text, add_start=True, add_end=True)\n                \n                # Convert to tensors\n                self.examples.append((\n                    torch.tensor(src_indices, dtype=torch.long),\n                    torch.tensor(tgt_indices, dtype=torch.long)\n                ))\n        else:\n            raise ValueError(f\"Unsupported dataset format: {dataset_format}\")\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, idx):\n        return self.examples[idx]\n\n\ndef read_transliteration_pairs(path):\n    \"\"\"Read transliteration pairs from tsv file\"\"\"\n    with open(path, encoding='utf-8') as f:\n        for line in f:\n            parts = line.strip().split('\\t')\n            if len(parts) >= 2:\n                yield parts[1], parts[0]  # Dakshina has target, source order\n\n\ndef read_transliteration_csv(path, src_col='src', tgt_col='trg'):\n    \"\"\"Read transliteration pairs from CSV file\"\"\"\n    df = pd.read_csv(path)\n    for _, row in df.iterrows():\n        yield row[src_col], row[tgt_col]\n\n\ndef batch_collate_function(batch, src_vocab, tgt_vocab):\n    \"\"\"Collate function for variable-length sequences\"\"\"\n    src_sequences, tgt_sequences = zip(*batch)\n    \n    # Pad sequences to equal length in batch\n    src_padded = pad_sequence(src_sequences, batch_first=True, padding_value=src_vocab.pad_idx)\n    tgt_padded = pad_sequence(tgt_sequences, batch_first=True, padding_value=tgt_vocab.pad_idx)\n    \n    # Create length tensor for packed sequences\n    src_lengths = torch.tensor([len(seq) for seq in src_sequences], dtype=torch.long)\n    \n    return src_padded, src_lengths, tgt_padded\n\n\ndef prepare_data_loaders(\n        language='ta',  # Changed from 'te' to 'ta' for Tamil\n        dataset_format='dakshina',\n        base_path=None,\n        batch_size=64,\n        device='cpu',\n        num_workers=2,\n        prefetch_factor=4,\n        persistent_workers=True,\n        cache_dir='./cache',\n        use_cached_vocab=True\n    ):\n    \"\"\"\n    Load datasets and create data loaders\n    \n    Args:\n        language: Language code ('ta' for Tamil)\n        dataset_format: Format specification ('dakshina')\n        base_path: Override default dataset path\n        batch_size: Batch size for loaders\n        device: Device to use ('cuda' or 'cpu')\n        num_workers: Data loading worker count\n        prefetch_factor: Number of batches to prefetch\n        persistent_workers: Keep workers alive between epochs\n        cache_dir: Directory for cached vocabularies\n        use_cached_vocab: Whether to use cached vocab files\n    \"\"\"\n    # Set up data paths\n    if base_path is None:\n        base_path = os.path.join(\n            '/kaggle/working/dakshina_dataset_v1.0',\n            language, 'lexicons'\n        )\n\n    # Set up vocabulary caching\n    if use_cached_vocab:\n        os.makedirs(cache_dir, exist_ok=True)\n        vocab_cache_path = os.path.join(cache_dir, f\"{language}_{dataset_format}_vocab.pkl\")\n    \n    # Try to load cached vocabularies\n    if use_cached_vocab and os.path.exists(vocab_cache_path):\n        print(f\"Loading cached vocabularies from {vocab_cache_path}\")\n        with open(vocab_cache_path, 'rb') as f:\n            src_vocab, tgt_vocab = pickle.load(f)\n    else:\n        # Build vocabularies from data\n        src_texts, tgt_texts = [], []\n        \n        for split in ['train', 'dev']:\n            data_path = os.path.join(base_path, f\"{language}.translit.sampled.{split}.tsv\")\n            for src, tgt in read_transliteration_pairs(data_path):\n                src_texts.append(src)\n                tgt_texts.append(tgt)\n        \n        # Create vocabularies\n        src_vocab = CharacterVocabulary.create_from_text_collection(src_texts)\n        tgt_vocab = CharacterVocabulary.create_from_text_collection(tgt_texts)\n        \n        # Cache for future use\n        if use_cached_vocab:\n            with open(vocab_cache_path, 'wb') as f:\n                pickle.dump((src_vocab, tgt_vocab), f)\n    \n    # Configure data loaders\n    loader_kwargs = dict(\n        batch_size=batch_size,\n        num_workers=num_workers,\n        prefetch_factor=prefetch_factor,\n        persistent_workers=persistent_workers and num_workers > 0,\n        pin_memory=(device == 'cuda')\n    )\n    \n    # Create data loaders for each split\n    data_loaders = {}\n    \n    splits = {'train': 'train', 'dev': 'dev', 'test': 'test'}\n    for split_name, file_suffix in splits.items():\n        data_path = os.path.join(base_path, f\"{language}.translit.sampled.{file_suffix}.tsv\")\n        dataset = TransliterationDataset(data_path, src_vocab, tgt_vocab, format='dakshina')\n        \n        data_loaders[split_name] = DataLoader(\n            dataset,\n            shuffle=(split_name == 'train'),\n            collate_fn=lambda b: batch_collate_function(b, src_vocab, tgt_vocab),\n            **loader_kwargs\n        )\n    \n    return data_loaders, src_vocab, tgt_vocab\n\n\n# Training and evaluation utilities\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport wandb\nfrom tqdm.auto import tqdm\nimport csv\nimport pandas as pd\n\ndef evaluate_model_accuracy(model, data_loader, target_vocab, source_vocab, device):\n    \"\"\"\n    Evaluate model prediction accuracy with detailed error analysis\n    \n    Returns:\n        - Overall accuracy\n        - Correct predictions details\n        - Incorrect predictions details\n    \"\"\"\n    model.eval()\n    correct_count = total_count = 0\n    \n    # Lists for detailed analysis\n    correct_sources = []\n    correct_targets = []\n    correct_predictions = []\n    \n    incorrect_sources = []\n    incorrect_targets = []\n    incorrect_predictions = []\n    \n    with torch.no_grad():\n        for src_batch, src_lengths, tgt_batch in data_loader:\n            # Move tensors to device\n            src_batch = src_batch.to(device)\n            src_lengths = src_lengths.to(device)\n            tgt_batch = tgt_batch.to(device)\n            \n            # Generate predictions\n            predictions = model.generate_greedy(\n                src_batch, src_lengths, target_vocab, max_length=tgt_batch.size(1)\n            )\n\n            # Evaluate each example in batch\n            for idx in range(src_batch.size(0)):\n                # Convert tensors to text\n                pred_text = target_vocab.decode_indices(predictions[idx].cpu())\n                gold_text = target_vocab.decode_indices(tgt_batch[idx, 1:].cpu())  # Skip <sos>\n                src_text = source_vocab.decode_indices(src_batch[idx].cpu())\n                \n                # Check accuracy\n                is_correct = (pred_text == gold_text)\n                correct_count += int(is_correct)\n                \n                # Store details for analysis\n                if is_correct:\n                    correct_sources.append(src_text)\n                    correct_targets.append(gold_text)\n                    correct_predictions.append(pred_text)\n                else:\n                    incorrect_sources.append(src_text)\n                    incorrect_targets.append(gold_text)\n                    incorrect_predictions.append(pred_text)\n                    \n            total_count += src_batch.size(0)\n\n    # Calculate overall accuracy\n    accuracy = correct_count / total_count if total_count > 0 else 0.0\n    \n    return (\n        accuracy, \n        (correct_sources, correct_targets, correct_predictions),\n        (incorrect_sources, incorrect_targets, incorrect_predictions)\n    )\n\ndef save_prediction_results(src_texts, tgt_texts, pred_texts, filename):\n    \"\"\"Save prediction details to CSV file\"\"\"\n    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Source', 'Target', 'Predicted'])\n        for row in zip(src_texts, tgt_texts, pred_texts):\n            writer.writerow(row)\n    \n    return filename\n\ndef train_sequence_model(\n    model, \n    data_loaders, \n    src_vocab, \n    tgt_vocab, \n    device,\n    config,\n    model_save_path=None,\n    enable_wandb=True\n):\n    \"\"\"\n    Train and evaluate the sequence model\n    \n    Args:\n        model: The Seq2Seq model\n        data_loaders: Dictionary of data loaders\n        src_vocab, tgt_vocab: Source and target vocabularies\n        device: Computation device\n        config: Training configuration\n        model_save_path: Where to save best model\n        enable_wandb: Whether to log to Weights & Biases\n    \"\"\"\n    # Setup loss function - ignore padding in loss calculation\n    loss_function = nn.CrossEntropyLoss(ignore_index=tgt_vocab.pad_idx)\n    \n    # Configure optimizer\n    if config.optimizer.lower() == 'adam':\n        optimizer = optim.Adam(model.parameters(), lr=config.lr)\n    elif config.optimizer.lower() == 'nadam':\n        optimizer = optim.NAdam(model.parameters(), lr=config.lr)\n    else:\n        optimizer = optim.Adam(model.parameters(), lr=config.lr)\n    \n    # Track best model\n    best_validation_accuracy = 0.0\n    \n    # Training loop\n    for epoch in tqdm(range(1, config.epochs + 1), desc=\"Training Progress\", position=0):\n        # Training phase\n        model.train()\n        epoch_loss = 0.0\n\n        # Process training batches\n        train_iter = tqdm(data_loaders['train'], desc=f\"Epoch {epoch}\", leave=False, position=1)\n        for src_batch, src_lengths, tgt_batch in train_iter:\n            # Move data to device\n            src_batch = src_batch.to(device)\n            src_lengths = src_lengths.to(device)\n            tgt_batch = tgt_batch.to(device)\n\n            # Forward and backward pass\n            optimizer.zero_grad()\n            batch_output = model(\n                src_batch, src_lengths, tgt_batch, \n                teacher_forcing_ratio=config.teacher_forcing\n            )\n            \n            # Calculate loss (flatten predictions and targets)\n            loss = loss_function(\n                batch_output.reshape(-1, batch_output.size(-1)), \n                tgt_batch[:,1:].reshape(-1)  # Skip <sos> token\n            )\n            \n            # Backpropagation\n            loss.backward()\n            \n            # Gradient clipping to prevent explosion\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            \n            # Update weights\n            optimizer.step()\n            \n            # Track loss\n            epoch_loss += loss.item()\n            \n        # Calculate average training loss\n        train_loss = epoch_loss / len(data_loaders['train'])\n\n        # Validation phase\n        val_loss = 0.0\n        val_iter = tqdm(data_loaders['dev'], desc=f\"Validation {epoch}\", leave=False, position=1)\n        \n        model.eval()\n        with torch.no_grad():\n            for src_batch, src_lengths, tgt_batch in val_iter:\n                # Move data to device\n                src_batch = src_batch.to(device)\n                src_lengths = src_lengths.to(device)\n                tgt_batch = tgt_batch.to(device)\n                \n                # Forward pass (no teacher forcing during validation)\n                batch_output = model(src_batch, src_lengths, tgt_batch, teacher_forcing_ratio=0.0)\n                \n                # Calculate validation loss\n                val_loss += loss_function(\n                    batch_output.reshape(-1, batch_output.size(-1)),\n                    tgt_batch[:,1:].reshape(-1)\n                ).item()\n        \n        # Calculate average validation loss\n        val_loss /= len(data_loaders['dev'])\n\n        # Compute accuracy metrics\n        train_results = evaluate_model_accuracy(\n            model, data_loaders['train'], tgt_vocab, src_vocab, device\n        )\n        train_accuracy = train_results[0]\n        \n        val_results = evaluate_model_accuracy(\n            model, data_loaders['dev'], tgt_vocab, src_vocab, device\n        )\n        val_accuracy = val_results[0]\n        \n        # Save best model\n        if val_accuracy > best_validation_accuracy and model_save_path:\n            best_validation_accuracy = val_accuracy\n            torch.save(model.state_dict(), model_save_path)\n            print(f\"✓ New best model saved with validation accuracy: {val_accuracy:.4f}\")\n            \n            # Save prediction analysis for best model\n            if epoch == config.epochs or epoch % 5 == 0:\n                # Save correct and incorrect predictions\n                correct_data = val_results[1]\n                incorrect_data = val_results[2]\n                \n                save_prediction_results(\n                    correct_data[0], correct_data[1], correct_data[2],\n                    f\"correct_preds_epoch_{epoch}.csv\"\n                )\n                \n                save_prediction_results(\n                    incorrect_data[0], incorrect_data[1], incorrect_data[2],\n                    f\"incorrect_preds_epoch_{epoch}.csv\"\n                )\n\n        # Print progress\n        print(f\"Epoch {epoch}/{config.epochs}:\")\n        print(f\"  Training:   Loss={train_loss:.4f}, Accuracy={train_accuracy:.4f}\")\n        print(f\"  Validation: Loss={val_loss:.4f}, Accuracy={val_accuracy:.4f}\")\n        \n        # Log metrics to WandB\n        if enable_wandb:\n            wandb.log({\n                'epoch': epoch,\n                'train_loss': train_loss,\n                'val_loss': val_loss,\n                'train_accuracy': train_accuracy,\n                'val_accuracy': val_accuracy\n            })\n    \n    # Placeholder for test accuracy (not evaluating test set in this version)\n    test_accuracy = 0\n    return model, test_accuracy\n\n\n# Hyperparameter sweeping functionality \nimport wandb\nimport torch\nfrom tqdm.auto import tqdm\nimport os\nimport random\nimport numpy as np\n\ndef objective():\n    # Initialize WandB run\n    run = wandb.init()\n    cfg = run.config\n    \n    # Set seeds for reproducibility\n    initialize_random_seeds(cfg.seed if hasattr(cfg, 'seed') else 42)\n    \n    # Set device\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    print(f\"Using device: {device}\")\n    \n    # Create a unique run name based on config\n    run_name = f\"{cfg.cell}_{cfg.enc_layers}l_{cfg.emb_size}e_{cfg.hidden_size}h_\" \\\n               f\"{'bid' if cfg.bidirectional else 'uni'}_{cfg.dropout}d_\" \\\n               f\"{cfg.teacher_forcing}tf_{cfg.optimizer}\"\n    wandb.run.name = run_name\n    \n    # Load data with Tamil language code\n    data_loaders, src_vocab, tgt_vocab = prepare_data_loaders(\n        'ta',  # Tamil language code\n        batch_size=cfg.batch_size,\n        device=device\n    )\n    \n    # Create encoder\n    encoder = EncoderNetwork(\n        src_vocab.size, cfg.emb_size, cfg.hidden_size,\n        cfg.enc_layers, cfg.cell, cfg.dropout, \n        use_bidirectional=cfg.bidirectional\n    ).to(device)\n    \n    # Calculate encoder output dimension\n    enc_output_dim = cfg.hidden_size * 2 if cfg.bidirectional else cfg.hidden_size\n    \n    # Create decoder\n    decoder = DecoderNetwork(\n        tgt_vocab.size, cfg.emb_size, enc_output_dim, cfg.hidden_size,\n        cfg.enc_layers, cfg.cell, cfg.dropout\n    ).to(device)\n    \n    # Combine into full model\n    model = Seq2SeqModel(encoder, decoder, pad_idx=src_vocab.pad_idx, device=device).to(device)\n    \n    # Train the model\n    best_model_path = f\"model_{run_name}.pt\"\n    _, test_accuracy = train_sequence_model(\n        model=model,\n        data_loaders=data_loaders,\n        src_vocab=src_vocab,\n        tgt_vocab=tgt_vocab,\n        device=device,\n        config=cfg,\n        model_save_path=best_model_path,\n        enable_wandb=True\n    )\n    \n    # Finish the run\n    wandb.finish()\n\nif __name__ == \"__main__\":\n    # Define sweep configuration - keeping hyperparameters the same as original\n    sweep_cfg = {\n        'method': 'bayes',  # Use Bayesian optimization\n        'name':'Tamil_Transliteration_with_Attention',\n        'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n        'parameters': {\n            \n            # Model architecture\n            'emb_size': {'values': [32,64,128, 256, 512]},\n            'hidden_size': {'values': [32,64,128, 256, 512, 1024]},\n            'enc_layers': {'values': [1, 2, 3, 4]},\n            'cell': {'values': ['RNN', 'GRU', 'LSTM']},  \n            'bidirectional': {'values': [True, False]},  # Bidirectional encode\n            \n            # Training parameters\n            'dropout': {'values': [0.0, 0.1, 0.2, 0.3, 0.5]},\n            'lr': {'values': [1e-4, 2e-4, 5e-4, 8e-4, 1e-3]},\n            'batch_size': {'values': [32, 64, 128]},\n            'epochs': {'values': [10, 15, 20]},\n            'teacher_forcing': {'values': [0.3, 0.5, 0.7, 1.0]},  # Explicit teacher forcing\n            'optimizer': {'values': ['Adam', 'NAdam']},  # Added optimizer options\n            # Reproducibility\n            'seed': {'values': [42, 43, 44, 45, 46]},  # Different seeds for robustness\n        }\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:10:45.640246Z","iopub.execute_input":"2025-05-20T19:10:45.640757Z","iopub.status.idle":"2025-05-20T19:10:51.891211Z","shell.execute_reply.started":"2025-05-20T19:10:45.640731Z","shell.execute_reply":"2025-05-20T19:10:51.890368Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"sweep_id = wandb.sweep(\n    sweep_cfg,\n    project='DA6401_A3'\n)\n\n# Run sweep agent\nwandb.agent(sweep_id, function=objective, count=30)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}